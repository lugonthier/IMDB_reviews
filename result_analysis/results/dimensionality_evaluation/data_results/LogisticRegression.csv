Run ID,Name,Source Type,Source Name,User,Status,memory,model,model__C,model__class_weight,model__dual,model__fit_intercept,model__intercept_scaling,model__l1_ratio,model__max_iter,model__multi_class,model__n_jobs,model__penalty,model__random_state,model__solver,model__tol,model__verbose,model__warm_start,preprocessor,preprocessor__normalization,preprocessor__stopwords,steps,training_size,vect,vect__max_features,vect__ngram_range,vect__vectorizer,vect__vectorizer__analyzer,vect__vectorizer__binary,vect__vectorizer__decode_error,vect__vectorizer__dtype,vect__vectorizer__encoding,vect__vectorizer__input,vect__vectorizer__lowercase,vect__vectorizer__max_df,vect__vectorizer__max_features,vect__vectorizer__min_df,vect__vectorizer__ngram_range,vect__vectorizer__norm,vect__vectorizer__preprocessor,vect__vectorizer__smooth_idf,vect__vectorizer__stop_words,vect__vectorizer__strip_accents,vect__vectorizer__sublinear_tf,vect__vectorizer__token_pattern,vect__vectorizer__tokenizer,vect__vectorizer__use_idf,vect__vectorizer__vocabulary,verbose,fit_time_mean,fit_time_std,predict_time_mean,predict_time_std,train_accuracy_score_mean,train_accuracy_score_std,train_f1_score_mean,train_f1_score_std,train_roc_auc_score_mean,train_roc_auc_score_std,valid_accuracy_score_mean,valid_accuracy_score_std,valid_f1_score_mean,valid_f1_score_std,valid_roc_auc_score_mean,valid_roc_auc_score_std
ef357c85b21e4bdf85903c866173fab6,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,LogisticRegression(max_iter=500),1.0,None,False,True,1,None,500,auto,None,l2,None,lbfgs,0.0001,0,False,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'should', 'had', 'under', 'shan', 'shouldn', 'theirs', 'don', ""it's"", 'hers', 'until', 'his', 'in', 'you', 'into', 's', 'what', 'off', 'such', 'than', 'be', 'any', 'or', 'ain', 'some', 'for', ""you'll"", ""mustn't"", 'again', 'ma', 'ourselves', 'just', 'we', 'before', 'other', 'each', 'needn', 'have', ""won't"", ""you'd"", ""you're"", 'can', ""that'll"", 'your', ""haven't"", ""isn't"", 'there', 'not', ""hadn't"", 'd', 'my', ""shouldn't"", 'll', 'didn', 'are', 'wouldn', 'as', ""don't"", 'so', 'hadn', 'with', 'both', 'out', 'own', 'further', ""hasn't"", 'weren', 'same', 're', 'a', 'o', 'were', 'i', 'haven', ""didn't"", 'our', 'him', 'they', 'here', ""mightn't"", 'those', 'up', 'of', 'she', 'is', 'how', 'about', 'more', 'them', 'an', ""couldn't"", 'where', 'wasn', 'below', 'against', 'am', ""she's"", ""should've"", 'couldn', 'doesn', 'above', 'herself', 'being', 'doing', 'at', ""aren't"", ""weren't"", 'm', ""shan't"", 'yourselves', ""wouldn't"", 'by', 'hasn', 'themselves', ""you've"", 'having', 'he', 'me', 'mightn', 't', 'her', 'few', 'over', 'will', 'isn', 'once', 'no', 'nor', 'too', 'while', 'who', ""wasn't"", 'ours', 'was', 'been', 'whom', 'has', 'did', 'when', 'himself', 'does', 'after', 'between', ""needn't"", 'because', 'aren', 'through', 'all', 'their', 'to', 'yourself', 'on', 'why', 'only', 'which', 'that', 'then', 'mustn', 'its', 'it', 'but', 'y', 'yours', 'itself', 'from', 'these', 'most', 'myself', 'do', 'now', 'very', 'if', 'won', 'this', ""doesn't"", 'down', 've', 'during', 'the', 'and'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=64000, vectorizer=TfidfVectorizer(max_features=64000))), ('model', LogisticRegression(max_iter=500))]",32000,"Vectorizer(max_features=64000, vectorizer=TfidfVectorizer(max_features=64000))",64000,"(1, 1)",TfidfVectorizer(max_features=64000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,64000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,71.4792293548584,0.460066312769223,17.062160539627076,0.2874451825179408,0.92895,0.0006475916923494361,0.929372890210059,0.0006412667750285269,0.9290474408862497,0.000645884345385784,0.89035,0.002640312481506711,0.8914166710336602,0.0027091107895977264,0.8905706126713776,0.00265556630665547
3fbb3a87bad949f1965d5cb60c02d8bc,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,LogisticRegression(max_iter=500),1.0,None,False,True,1,None,500,auto,None,l2,None,lbfgs,0.0001,0,False,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'should', 'had', 'under', 'shan', 'shouldn', 'theirs', 'don', ""it's"", 'hers', 'until', 'his', 'in', 'you', 'into', 's', 'what', 'off', 'such', 'than', 'be', 'any', 'or', 'ain', 'some', 'for', ""you'll"", ""mustn't"", 'again', 'ma', 'ourselves', 'just', 'we', 'before', 'other', 'each', 'needn', 'have', ""won't"", ""you'd"", ""you're"", 'can', ""that'll"", 'your', ""haven't"", ""isn't"", 'there', 'not', ""hadn't"", 'd', 'my', ""shouldn't"", 'll', 'didn', 'are', 'wouldn', 'as', ""don't"", 'so', 'hadn', 'with', 'both', 'out', 'own', 'further', ""hasn't"", 'weren', 'same', 're', 'a', 'o', 'were', 'i', 'haven', ""didn't"", 'our', 'him', 'they', 'here', ""mightn't"", 'those', 'up', 'of', 'she', 'is', 'how', 'about', 'more', 'them', 'an', ""couldn't"", 'where', 'wasn', 'below', 'against', 'am', ""she's"", ""should've"", 'couldn', 'doesn', 'above', 'herself', 'being', 'doing', 'at', ""aren't"", ""weren't"", 'm', ""shan't"", 'yourselves', ""wouldn't"", 'by', 'hasn', 'themselves', ""you've"", 'having', 'he', 'me', 'mightn', 't', 'her', 'few', 'over', 'will', 'isn', 'once', 'no', 'nor', 'too', 'while', 'who', ""wasn't"", 'ours', 'was', 'been', 'whom', 'has', 'did', 'when', 'himself', 'does', 'after', 'between', ""needn't"", 'because', 'aren', 'through', 'all', 'their', 'to', 'yourself', 'on', 'why', 'only', 'which', 'that', 'then', 'mustn', 'its', 'it', 'but', 'y', 'yours', 'itself', 'from', 'these', 'most', 'myself', 'do', 'now', 'very', 'if', 'won', 'this', ""doesn't"", 'down', 've', 'during', 'the', 'and'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=32000, vectorizer=TfidfVectorizer(max_features=32000))), ('model', LogisticRegression(max_iter=500))]",32000,"Vectorizer(max_features=32000, vectorizer=TfidfVectorizer(max_features=32000))",32000,"(1, 1)",TfidfVectorizer(max_features=32000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,32000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,71.02536244392395,0.5202986631782399,17.00425500869751,0.2012762343877949,0.9274749999999999,0.0006562202374203327,0.9279113677322848,0.0006242152573382265,0.92757408330455,0.0006450347147366358,0.8903749999999999,0.0023690715480964293,0.8914490400807054,0.0024619596051243,0.8906049612098217,0.0023825693338712707
b79df4aa3dac45519fa0eda8b1bc89f3,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,LogisticRegression(max_iter=500),1.0,None,False,True,1,None,500,auto,None,l2,None,lbfgs,0.0001,0,False,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'should', 'had', 'under', 'shan', 'shouldn', 'theirs', 'don', ""it's"", 'hers', 'until', 'his', 'in', 'you', 'into', 's', 'what', 'off', 'such', 'than', 'be', 'any', 'or', 'ain', 'some', 'for', ""you'll"", ""mustn't"", 'again', 'ma', 'ourselves', 'just', 'we', 'before', 'other', 'each', 'needn', 'have', ""won't"", ""you'd"", ""you're"", 'can', ""that'll"", 'your', ""haven't"", ""isn't"", 'there', 'not', ""hadn't"", 'd', 'my', ""shouldn't"", 'll', 'didn', 'are', 'wouldn', 'as', ""don't"", 'so', 'hadn', 'with', 'both', 'out', 'own', 'further', ""hasn't"", 'weren', 'same', 're', 'a', 'o', 'were', 'i', 'haven', ""didn't"", 'our', 'him', 'they', 'here', ""mightn't"", 'those', 'up', 'of', 'she', 'is', 'how', 'about', 'more', 'them', 'an', ""couldn't"", 'where', 'wasn', 'below', 'against', 'am', ""she's"", ""should've"", 'couldn', 'doesn', 'above', 'herself', 'being', 'doing', 'at', ""aren't"", ""weren't"", 'm', ""shan't"", 'yourselves', ""wouldn't"", 'by', 'hasn', 'themselves', ""you've"", 'having', 'he', 'me', 'mightn', 't', 'her', 'few', 'over', 'will', 'isn', 'once', 'no', 'nor', 'too', 'while', 'who', ""wasn't"", 'ours', 'was', 'been', 'whom', 'has', 'did', 'when', 'himself', 'does', 'after', 'between', ""needn't"", 'because', 'aren', 'through', 'all', 'their', 'to', 'yourself', 'on', 'why', 'only', 'which', 'that', 'then', 'mustn', 'its', 'it', 'but', 'y', 'yours', 'itself', 'from', 'these', 'most', 'myself', 'do', 'now', 'very', 'if', 'won', 'this', ""doesn't"", 'down', 've', 'during', 'the', 'and'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=16000, vectorizer=TfidfVectorizer(max_features=16000))), ('model', LogisticRegression(max_iter=500))]",32000,"Vectorizer(max_features=16000, vectorizer=TfidfVectorizer(max_features=16000))",16000,"(1, 1)",TfidfVectorizer(max_features=16000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,16000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,71.77501440048218,0.66693944088939,17.3302020072937,0.22127710520692676,0.9243500000000001,0.0006606696413488403,0.9248387661281811,0.0006289992450115541,0.9244603179543718,0.0006497034837974551,0.8899250000000001,0.002181742422927161,0.8910444979893514,0.002383072272327758,0.8901726146928031,0.002229287118674721
52162545d9854fe688f4d1ea7d2823f0,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,LogisticRegression(max_iter=500),1.0,None,False,True,1,None,500,auto,None,l2,None,lbfgs,0.0001,0,False,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'should', 'had', 'under', 'shan', 'shouldn', 'theirs', 'don', ""it's"", 'hers', 'until', 'his', 'in', 'you', 'into', 's', 'what', 'off', 'such', 'than', 'be', 'any', 'or', 'ain', 'some', 'for', ""you'll"", ""mustn't"", 'again', 'ma', 'ourselves', 'just', 'we', 'before', 'other', 'each', 'needn', 'have', ""won't"", ""you'd"", ""you're"", 'can', ""that'll"", 'your', ""haven't"", ""isn't"", 'there', 'not', ""hadn't"", 'd', 'my', ""shouldn't"", 'll', 'didn', 'are', 'wouldn', 'as', ""don't"", 'so', 'hadn', 'with', 'both', 'out', 'own', 'further', ""hasn't"", 'weren', 'same', 're', 'a', 'o', 'were', 'i', 'haven', ""didn't"", 'our', 'him', 'they', 'here', ""mightn't"", 'those', 'up', 'of', 'she', 'is', 'how', 'about', 'more', 'them', 'an', ""couldn't"", 'where', 'wasn', 'below', 'against', 'am', ""she's"", ""should've"", 'couldn', 'doesn', 'above', 'herself', 'being', 'doing', 'at', ""aren't"", ""weren't"", 'm', ""shan't"", 'yourselves', ""wouldn't"", 'by', 'hasn', 'themselves', ""you've"", 'having', 'he', 'me', 'mightn', 't', 'her', 'few', 'over', 'will', 'isn', 'once', 'no', 'nor', 'too', 'while', 'who', ""wasn't"", 'ours', 'was', 'been', 'whom', 'has', 'did', 'when', 'himself', 'does', 'after', 'between', ""needn't"", 'because', 'aren', 'through', 'all', 'their', 'to', 'yourself', 'on', 'why', 'only', 'which', 'that', 'then', 'mustn', 'its', 'it', 'but', 'y', 'yours', 'itself', 'from', 'these', 'most', 'myself', 'do', 'now', 'very', 'if', 'won', 'this', ""doesn't"", 'down', 've', 'during', 'the', 'and'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=8000, vectorizer=TfidfVectorizer(max_features=8000))), ('model', LogisticRegression(max_iter=500))]",32000,"Vectorizer(max_features=8000, vectorizer=TfidfVectorizer(max_features=8000))",8000,"(1, 1)",TfidfVectorizer(max_features=8000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,8000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,70.25786628723145,0.9070140808886066,17.052465963363648,0.4309014481766092,0.9181374999999999,0.0004899776780629972,0.9186642899822145,0.0004609079672874188,0.918245346740713,0.00048066808794303774,0.8883749999999999,0.002210769096943426,0.8893844073342111,0.002425292013286388,0.8885744437033474,0.002264123750569164
caded20f8eb84f36ac5a768f21cf843e,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,LogisticRegression(max_iter=500),1.0,None,False,True,1,None,500,auto,None,l2,None,lbfgs,0.0001,0,False,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'should', 'had', 'under', 'shan', 'shouldn', 'theirs', 'don', ""it's"", 'hers', 'until', 'his', 'in', 'you', 'into', 's', 'what', 'off', 'such', 'than', 'be', 'any', 'or', 'ain', 'some', 'for', ""you'll"", ""mustn't"", 'again', 'ma', 'ourselves', 'just', 'we', 'before', 'other', 'each', 'needn', 'have', ""won't"", ""you'd"", ""you're"", 'can', ""that'll"", 'your', ""haven't"", ""isn't"", 'there', 'not', ""hadn't"", 'd', 'my', ""shouldn't"", 'll', 'didn', 'are', 'wouldn', 'as', ""don't"", 'so', 'hadn', 'with', 'both', 'out', 'own', 'further', ""hasn't"", 'weren', 'same', 're', 'a', 'o', 'were', 'i', 'haven', ""didn't"", 'our', 'him', 'they', 'here', ""mightn't"", 'those', 'up', 'of', 'she', 'is', 'how', 'about', 'more', 'them', 'an', ""couldn't"", 'where', 'wasn', 'below', 'against', 'am', ""she's"", ""should've"", 'couldn', 'doesn', 'above', 'herself', 'being', 'doing', 'at', ""aren't"", ""weren't"", 'm', ""shan't"", 'yourselves', ""wouldn't"", 'by', 'hasn', 'themselves', ""you've"", 'having', 'he', 'me', 'mightn', 't', 'her', 'few', 'over', 'will', 'isn', 'once', 'no', 'nor', 'too', 'while', 'who', ""wasn't"", 'ours', 'was', 'been', 'whom', 'has', 'did', 'when', 'himself', 'does', 'after', 'between', ""needn't"", 'because', 'aren', 'through', 'all', 'their', 'to', 'yourself', 'on', 'why', 'only', 'which', 'that', 'then', 'mustn', 'its', 'it', 'but', 'y', 'yours', 'itself', 'from', 'these', 'most', 'myself', 'do', 'now', 'very', 'if', 'won', 'this', ""doesn't"", 'down', 've', 'during', 'the', 'and'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=4000, vectorizer=TfidfVectorizer(max_features=4000))), ('model', LogisticRegression(max_iter=500))]",32000,"Vectorizer(max_features=4000, vectorizer=TfidfVectorizer(max_features=4000))",4000,"(1, 1)",TfidfVectorizer(max_features=4000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,4000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,69.12048559188842,0.2839079278976792,16.76906862258911,0.13073317435312734,0.9080124999999999,0.0007045055890480897,0.9086861887373459,0.0006636927178986844,0.9081433129923259,0.0006928177348893808,0.8842500000000001,0.003141456668489949,0.8852982759506982,0.0033427906316374108,0.8844439797931001,0.003199057294590187
b47d4fa06cbe48e39fa5f3154f020d52,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,LogisticRegression(max_iter=500),1.0,None,False,True,1,None,500,auto,None,l2,None,lbfgs,0.0001,0,False,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'should', 'had', 'under', 'shan', 'shouldn', 'theirs', 'don', ""it's"", 'hers', 'until', 'his', 'in', 'you', 'into', 's', 'what', 'off', 'such', 'than', 'be', 'any', 'or', 'ain', 'some', 'for', ""you'll"", ""mustn't"", 'again', 'ma', 'ourselves', 'just', 'we', 'before', 'other', 'each', 'needn', 'have', ""won't"", ""you'd"", ""you're"", 'can', ""that'll"", 'your', ""haven't"", ""isn't"", 'there', 'not', ""hadn't"", 'd', 'my', ""shouldn't"", 'll', 'didn', 'are', 'wouldn', 'as', ""don't"", 'so', 'hadn', 'with', 'both', 'out', 'own', 'further', ""hasn't"", 'weren', 'same', 're', 'a', 'o', 'were', 'i', 'haven', ""didn't"", 'our', 'him', 'they', 'here', ""mightn't"", 'those', 'up', 'of', 'she', 'is', 'how', 'about', 'more', 'them', 'an', ""couldn't"", 'where', 'wasn', 'below', 'against', 'am', ""she's"", ""should've"", 'couldn', 'doesn', 'above', 'herself', 'being', 'doing', 'at', ""aren't"", ""weren't"", 'm', ""shan't"", 'yourselves', ""wouldn't"", 'by', 'hasn', 'themselves', ""you've"", 'having', 'he', 'me', 'mightn', 't', 'her', 'few', 'over', 'will', 'isn', 'once', 'no', 'nor', 'too', 'while', 'who', ""wasn't"", 'ours', 'was', 'been', 'whom', 'has', 'did', 'when', 'himself', 'does', 'after', 'between', ""needn't"", 'because', 'aren', 'through', 'all', 'their', 'to', 'yourself', 'on', 'why', 'only', 'which', 'that', 'then', 'mustn', 'its', 'it', 'but', 'y', 'yours', 'itself', 'from', 'these', 'most', 'myself', 'do', 'now', 'very', 'if', 'won', 'this', ""doesn't"", 'down', 've', 'during', 'the', 'and'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=2000, vectorizer=TfidfVectorizer(max_features=2000))), ('model', LogisticRegression(max_iter=500))]",32000,"Vectorizer(max_features=2000, vectorizer=TfidfVectorizer(max_features=2000))",2000,"(1, 1)",TfidfVectorizer(max_features=2000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,2000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,69.13654055595399,0.3843065839617235,16.754375648498534,0.09949830259691105,0.8936250000000001,0.0004437059837324724,0.894344843870269,0.00041254305579112896,0.893737112526362,0.00043521746080228094,0.875475,0.0028044607324760463,0.8763948470965224,0.0031765346573665813,0.8756209024570125,0.0028896612512275557
933b3229bdd64c9f90c69fa04e713b72,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,LogisticRegression(max_iter=500),1.0,None,False,True,1,None,500,auto,None,l2,None,lbfgs,0.0001,0,False,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'should', 'had', 'under', 'shan', 'shouldn', 'theirs', 'don', ""it's"", 'hers', 'until', 'his', 'in', 'you', 'into', 's', 'what', 'off', 'such', 'than', 'be', 'any', 'or', 'ain', 'some', 'for', ""you'll"", ""mustn't"", 'again', 'ma', 'ourselves', 'just', 'we', 'before', 'other', 'each', 'needn', 'have', ""won't"", ""you'd"", ""you're"", 'can', ""that'll"", 'your', ""haven't"", ""isn't"", 'there', 'not', ""hadn't"", 'd', 'my', ""shouldn't"", 'll', 'didn', 'are', 'wouldn', 'as', ""don't"", 'so', 'hadn', 'with', 'both', 'out', 'own', 'further', ""hasn't"", 'weren', 'same', 're', 'a', 'o', 'were', 'i', 'haven', ""didn't"", 'our', 'him', 'they', 'here', ""mightn't"", 'those', 'up', 'of', 'she', 'is', 'how', 'about', 'more', 'them', 'an', ""couldn't"", 'where', 'wasn', 'below', 'against', 'am', ""she's"", ""should've"", 'couldn', 'doesn', 'above', 'herself', 'being', 'doing', 'at', ""aren't"", ""weren't"", 'm', ""shan't"", 'yourselves', ""wouldn't"", 'by', 'hasn', 'themselves', ""you've"", 'having', 'he', 'me', 'mightn', 't', 'her', 'few', 'over', 'will', 'isn', 'once', 'no', 'nor', 'too', 'while', 'who', ""wasn't"", 'ours', 'was', 'been', 'whom', 'has', 'did', 'when', 'himself', 'does', 'after', 'between', ""needn't"", 'because', 'aren', 'through', 'all', 'their', 'to', 'yourself', 'on', 'why', 'only', 'which', 'that', 'then', 'mustn', 'its', 'it', 'but', 'y', 'yours', 'itself', 'from', 'these', 'most', 'myself', 'do', 'now', 'very', 'if', 'won', 'this', ""doesn't"", 'down', 've', 'during', 'the', 'and'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=1000, vectorizer=TfidfVectorizer(max_features=1000))), ('model', LogisticRegression(max_iter=500))]",32000,"Vectorizer(max_features=1000, vectorizer=TfidfVectorizer(max_features=1000))",1000,"(1, 1)",TfidfVectorizer(max_features=1000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,1000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,69.99410676956177,1.1966220501471427,16.88286499977112,0.1711860057622864,0.8742749999999999,0.0008388515214267811,0.8753062922355588,0.0009050447425522172,0.8744220988417416,0.0008573734824799958,0.862025,0.00221979728804232,0.8631071084365205,0.002326570585927614,0.8621722271219248,0.002233074465788302
5eff0fa02fe14b16a700fd9875b8aaca,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,LogisticRegression(max_iter=500),1.0,None,False,True,1,None,500,auto,None,l2,None,lbfgs,0.0001,0,False,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'should', 'had', 'under', 'shan', 'shouldn', 'theirs', 'don', ""it's"", 'hers', 'until', 'his', 'in', 'you', 'into', 's', 'what', 'off', 'such', 'than', 'be', 'any', 'or', 'ain', 'some', 'for', ""you'll"", ""mustn't"", 'again', 'ma', 'ourselves', 'just', 'we', 'before', 'other', 'each', 'needn', 'have', ""won't"", ""you'd"", ""you're"", 'can', ""that'll"", 'your', ""haven't"", ""isn't"", 'there', 'not', ""hadn't"", 'd', 'my', ""shouldn't"", 'll', 'didn', 'are', 'wouldn', 'as', ""don't"", 'so', 'hadn', 'with', 'both', 'out', 'own', 'further', ""hasn't"", 'weren', 'same', 're', 'a', 'o', 'were', 'i', 'haven', ""didn't"", 'our', 'him', 'they', 'here', ""mightn't"", 'those', 'up', 'of', 'she', 'is', 'how', 'about', 'more', 'them', 'an', ""couldn't"", 'where', 'wasn', 'below', 'against', 'am', ""she's"", ""should've"", 'couldn', 'doesn', 'above', 'herself', 'being', 'doing', 'at', ""aren't"", ""weren't"", 'm', ""shan't"", 'yourselves', ""wouldn't"", 'by', 'hasn', 'themselves', ""you've"", 'having', 'he', 'me', 'mightn', 't', 'her', 'few', 'over', 'will', 'isn', 'once', 'no', 'nor', 'too', 'while', 'who', ""wasn't"", 'ours', 'was', 'been', 'whom', 'has', 'did', 'when', 'himself', 'does', 'after', 'between', ""needn't"", 'because', 'aren', 'through', 'all', 'their', 'to', 'yourself', 'on', 'why', 'only', 'which', 'that', 'then', 'mustn', 'its', 'it', 'but', 'y', 'yours', 'itself', 'from', 'these', 'most', 'myself', 'do', 'now', 'very', 'if', 'won', 'this', ""doesn't"", 'down', 've', 'during', 'the', 'and'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=500, vectorizer=TfidfVectorizer(max_features=500))), ('model', LogisticRegression(max_iter=500))]",32000,"Vectorizer(max_features=500, vectorizer=TfidfVectorizer(max_features=500))",500,"(1, 1)",TfidfVectorizer(max_features=500),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,500,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,70.26729626655579,0.9419867398996844,17.240792179107665,0.5334547956710012,0.8533125,0.0006425656581237559,0.854510885138923,0.0006499691496373327,0.8534509627896739,0.0006441030275181856,0.84605,0.0013661076092314546,0.847329507385085,0.001547970798493737,0.846210791947208,0.0013673973374899843
