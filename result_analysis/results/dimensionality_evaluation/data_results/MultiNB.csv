Run ID,Name,Source Type,Source Name,User,Status,memory,model,model__alpha,model__class_prior,model__fit_prior,preprocessor,preprocessor__normalization,preprocessor__stopwords,steps,training_size,vect,vect__max_features,vect__ngram_range,vect__vectorizer,vect__vectorizer__analyzer,vect__vectorizer__binary,vect__vectorizer__decode_error,vect__vectorizer__dtype,vect__vectorizer__encoding,vect__vectorizer__input,vect__vectorizer__lowercase,vect__vectorizer__max_df,vect__vectorizer__max_features,vect__vectorizer__min_df,vect__vectorizer__ngram_range,vect__vectorizer__norm,vect__vectorizer__preprocessor,vect__vectorizer__smooth_idf,vect__vectorizer__stop_words,vect__vectorizer__strip_accents,vect__vectorizer__sublinear_tf,vect__vectorizer__token_pattern,vect__vectorizer__tokenizer,vect__vectorizer__use_idf,vect__vectorizer__vocabulary,verbose,fit_time_mean,fit_time_std,predict_time_mean,predict_time_std,train_accuracy_score_mean,train_accuracy_score_std,train_f1_score_mean,train_f1_score_std,train_roc_auc_score_mean,train_roc_auc_score_std,valid_accuracy_score_mean,valid_accuracy_score_std,valid_f1_score_mean,valid_f1_score_std,valid_roc_auc_score_mean,valid_roc_auc_score_std
df55b25f77ff4c1bbb957f3df42be758,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,MultinomialNB(),1.0,None,True,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'so', 'too', 'during', 'won', 'what', 'as', 'did', 'with', 'against', 'below', 'off', 've', 'be', ""don't"", 'aren', 'if', 'itself', 'own', 'i', ""it's"", 'o', 'same', ""wasn't"", 'it', 'both', 'd', 'where', 'that', ""mustn't"", ""won't"", 'about', 'other', 'didn', ""you'd"", 'there', 'which', 'mustn', 'himself', 'by', 'wasn', 'into', 'of', 'them', 'yourself', 'couldn', 'having', 'had', 'up', 'only', 'herself', 'just', 'very', 'who', 'her', 'hasn', 'me', 'this', 'themselves', 'while', 'how', ""that'll"", 'can', 'not', 'from', 'isn', 'being', 'our', 'once', 'been', 'have', 'such', 'they', 'his', 'because', 'over', 'mightn', 'does', 'few', 'y', ""should've"", 'myself', 'each', 'weren', 'my', 'he', 'no', ""hadn't"", 'whom', 'm', 'll', ""didn't"", 'shouldn', 'further', 'those', 'then', 'should', 'doing', 'theirs', 'yourselves', 'after', 'hers', ""couldn't"", ""isn't"", 'when', ""shan't"", 'than', 'ma', 'some', 'was', 'all', ""haven't"", 'more', 'him', 'she', 'doesn', ""hasn't"", ""she's"", 'needn', 'their', 'shan', 'you', ""needn't"", 'between', 'are', 'on', 'at', 'down', 'most', ""you've"", 'now', 'haven', ""shouldn't"", ""doesn't"", ""mightn't"", ""you're"", 're', 't', 'the', 'is', 'your', 'hadn', 'yours', 'an', ""you'll"", 'through', 'until', ""wouldn't"", 'and', 'has', 'were', ""weren't"", 'ours', 'here', 'above', 'but', 's', 'wouldn', 'before', 'do', 'to', 'why', 'out', 'we', 'or', 'any', 'for', 'in', 'ain', 'don', 'ourselves', 'these', ""aren't"", 'nor', 'am', 'again', 'its', 'will', 'a', 'under'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=64000, vectorizer=TfidfVectorizer(max_features=64000))), ('model', MultinomialNB())]",32000,"Vectorizer(max_features=64000, vectorizer=TfidfVectorizer(max_features=64000))",64000,"(1, 1)",TfidfVectorizer(max_features=64000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,64000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,71.14957938194274,0.5652786399337164,17.397704982757567,0.5109631861145055,0.90374375,0.00102553339779843,0.9022711321745414,0.0010361617763127078,0.9040379667166356,0.001029546511956551,0.860525,0.004443675280665759,0.8582924996447019,0.0046269534288758876,0.8608165429618768,0.004417249753603525
4dafa390958b4a7aaa589f9cfa0cfc06,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,MultinomialNB(),1.0,None,True,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'so', 'too', 'during', 'won', 'what', 'as', 'did', 'with', 'against', 'below', 'off', 've', 'be', ""don't"", 'aren', 'if', 'itself', 'own', 'i', ""it's"", 'o', 'same', ""wasn't"", 'it', 'both', 'd', 'where', 'that', ""mustn't"", ""won't"", 'about', 'other', 'didn', ""you'd"", 'there', 'which', 'mustn', 'himself', 'by', 'wasn', 'into', 'of', 'them', 'yourself', 'couldn', 'having', 'had', 'up', 'only', 'herself', 'just', 'very', 'who', 'her', 'hasn', 'me', 'this', 'themselves', 'while', 'how', ""that'll"", 'can', 'not', 'from', 'isn', 'being', 'our', 'once', 'been', 'have', 'such', 'they', 'his', 'because', 'over', 'mightn', 'does', 'few', 'y', ""should've"", 'myself', 'each', 'weren', 'my', 'he', 'no', ""hadn't"", 'whom', 'm', 'll', ""didn't"", 'shouldn', 'further', 'those', 'then', 'should', 'doing', 'theirs', 'yourselves', 'after', 'hers', ""couldn't"", ""isn't"", 'when', ""shan't"", 'than', 'ma', 'some', 'was', 'all', ""haven't"", 'more', 'him', 'she', 'doesn', ""hasn't"", ""she's"", 'needn', 'their', 'shan', 'you', ""needn't"", 'between', 'are', 'on', 'at', 'down', 'most', ""you've"", 'now', 'haven', ""shouldn't"", ""doesn't"", ""mightn't"", ""you're"", 're', 't', 'the', 'is', 'your', 'hadn', 'yours', 'an', ""you'll"", 'through', 'until', ""wouldn't"", 'and', 'has', 'were', ""weren't"", 'ours', 'here', 'above', 'but', 's', 'wouldn', 'before', 'do', 'to', 'why', 'out', 'we', 'or', 'any', 'for', 'in', 'ain', 'don', 'ourselves', 'these', ""aren't"", 'nor', 'am', 'again', 'its', 'will', 'a', 'under'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=32000, vectorizer=TfidfVectorizer(max_features=32000))), ('model', MultinomialNB())]",32000,"Vectorizer(max_features=32000, vectorizer=TfidfVectorizer(max_features=32000))",32000,"(1, 1)",TfidfVectorizer(max_features=32000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,32000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,72.96787881851196,0.4010462449059305,18.220632362365723,0.9458013739579898,0.896575,0.0010384408384689161,0.8951234176738323,0.001085574116852826,0.8968133501799533,0.001027737180253863,0.85975,0.005071365693775216,0.8578372020289329,0.0053362812738036265,0.8599556463929716,0.0050318961371637345
93ed0fa6c9dd4de6b7548aecb8d5a0c5,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,MultinomialNB(),1.0,None,True,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'so', 'too', 'during', 'won', 'what', 'as', 'did', 'with', 'against', 'below', 'off', 've', 'be', ""don't"", 'aren', 'if', 'itself', 'own', 'i', ""it's"", 'o', 'same', ""wasn't"", 'it', 'both', 'd', 'where', 'that', ""mustn't"", ""won't"", 'about', 'other', 'didn', ""you'd"", 'there', 'which', 'mustn', 'himself', 'by', 'wasn', 'into', 'of', 'them', 'yourself', 'couldn', 'having', 'had', 'up', 'only', 'herself', 'just', 'very', 'who', 'her', 'hasn', 'me', 'this', 'themselves', 'while', 'how', ""that'll"", 'can', 'not', 'from', 'isn', 'being', 'our', 'once', 'been', 'have', 'such', 'they', 'his', 'because', 'over', 'mightn', 'does', 'few', 'y', ""should've"", 'myself', 'each', 'weren', 'my', 'he', 'no', ""hadn't"", 'whom', 'm', 'll', ""didn't"", 'shouldn', 'further', 'those', 'then', 'should', 'doing', 'theirs', 'yourselves', 'after', 'hers', ""couldn't"", ""isn't"", 'when', ""shan't"", 'than', 'ma', 'some', 'was', 'all', ""haven't"", 'more', 'him', 'she', 'doesn', ""hasn't"", ""she's"", 'needn', 'their', 'shan', 'you', ""needn't"", 'between', 'are', 'on', 'at', 'down', 'most', ""you've"", 'now', 'haven', ""shouldn't"", ""doesn't"", ""mightn't"", ""you're"", 're', 't', 'the', 'is', 'your', 'hadn', 'yours', 'an', ""you'll"", 'through', 'until', ""wouldn't"", 'and', 'has', 'were', ""weren't"", 'ours', 'here', 'above', 'but', 's', 'wouldn', 'before', 'do', 'to', 'why', 'out', 'we', 'or', 'any', 'for', 'in', 'ain', 'don', 'ourselves', 'these', ""aren't"", 'nor', 'am', 'again', 'its', 'will', 'a', 'under'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=16000, vectorizer=TfidfVectorizer(max_features=16000))), ('model', MultinomialNB())]",32000,"Vectorizer(max_features=16000, vectorizer=TfidfVectorizer(max_features=16000))",16000,"(1, 1)",TfidfVectorizer(max_features=16000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,16000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,72.85174369812012,2.0206554517659545,17.802178287506102,0.536808036422043,0.8848437500000002,0.0011551515052148026,0.8837892201726574,0.0012237074217619852,0.8849304017454553,0.0011439347458325575,0.857775,0.005497954164959906,0.8565942603837993,0.005731378268825495,0.8578487881960927,0.0054875137340973925
2a051faab5814a7cada8fbe7fb294655,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,MultinomialNB(),1.0,None,True,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'so', 'too', 'during', 'won', 'what', 'as', 'did', 'with', 'against', 'below', 'off', 've', 'be', ""don't"", 'aren', 'if', 'itself', 'own', 'i', ""it's"", 'o', 'same', ""wasn't"", 'it', 'both', 'd', 'where', 'that', ""mustn't"", ""won't"", 'about', 'other', 'didn', ""you'd"", 'there', 'which', 'mustn', 'himself', 'by', 'wasn', 'into', 'of', 'them', 'yourself', 'couldn', 'having', 'had', 'up', 'only', 'herself', 'just', 'very', 'who', 'her', 'hasn', 'me', 'this', 'themselves', 'while', 'how', ""that'll"", 'can', 'not', 'from', 'isn', 'being', 'our', 'once', 'been', 'have', 'such', 'they', 'his', 'because', 'over', 'mightn', 'does', 'few', 'y', ""should've"", 'myself', 'each', 'weren', 'my', 'he', 'no', ""hadn't"", 'whom', 'm', 'll', ""didn't"", 'shouldn', 'further', 'those', 'then', 'should', 'doing', 'theirs', 'yourselves', 'after', 'hers', ""couldn't"", ""isn't"", 'when', ""shan't"", 'than', 'ma', 'some', 'was', 'all', ""haven't"", 'more', 'him', 'she', 'doesn', ""hasn't"", ""she's"", 'needn', 'their', 'shan', 'you', ""needn't"", 'between', 'are', 'on', 'at', 'down', 'most', ""you've"", 'now', 'haven', ""shouldn't"", ""doesn't"", ""mightn't"", ""you're"", 're', 't', 'the', 'is', 'your', 'hadn', 'yours', 'an', ""you'll"", 'through', 'until', ""wouldn't"", 'and', 'has', 'were', ""weren't"", 'ours', 'here', 'above', 'but', 's', 'wouldn', 'before', 'do', 'to', 'why', 'out', 'we', 'or', 'any', 'for', 'in', 'ain', 'don', 'ourselves', 'these', ""aren't"", 'nor', 'am', 'again', 'its', 'will', 'a', 'under'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=8000, vectorizer=TfidfVectorizer(max_features=8000))), ('model', MultinomialNB())]",32000,"Vectorizer(max_features=8000, vectorizer=TfidfVectorizer(max_features=8000))",8000,"(1, 1)",TfidfVectorizer(max_features=8000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,8000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,72.17415752410889,1.9550734990675736,17.464170122146605,0.41894240303661656,0.8703,0.0010234744745229415,0.8700853199871817,0.0010525002592260658,0.8703018619494504,0.0010214043512166363,0.852625,0.002196588263648873,0.8525495697490413,0.0024372825742462125,0.8526545059034694,0.00221395534505194
36fdf84462b240fd8163fb1db45d8696,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,MultinomialNB(),1.0,None,True,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'so', 'too', 'during', 'won', 'what', 'as', 'did', 'with', 'against', 'below', 'off', 've', 'be', ""don't"", 'aren', 'if', 'itself', 'own', 'i', ""it's"", 'o', 'same', ""wasn't"", 'it', 'both', 'd', 'where', 'that', ""mustn't"", ""won't"", 'about', 'other', 'didn', ""you'd"", 'there', 'which', 'mustn', 'himself', 'by', 'wasn', 'into', 'of', 'them', 'yourself', 'couldn', 'having', 'had', 'up', 'only', 'herself', 'just', 'very', 'who', 'her', 'hasn', 'me', 'this', 'themselves', 'while', 'how', ""that'll"", 'can', 'not', 'from', 'isn', 'being', 'our', 'once', 'been', 'have', 'such', 'they', 'his', 'because', 'over', 'mightn', 'does', 'few', 'y', ""should've"", 'myself', 'each', 'weren', 'my', 'he', 'no', ""hadn't"", 'whom', 'm', 'll', ""didn't"", 'shouldn', 'further', 'those', 'then', 'should', 'doing', 'theirs', 'yourselves', 'after', 'hers', ""couldn't"", ""isn't"", 'when', ""shan't"", 'than', 'ma', 'some', 'was', 'all', ""haven't"", 'more', 'him', 'she', 'doesn', ""hasn't"", ""she's"", 'needn', 'their', 'shan', 'you', ""needn't"", 'between', 'are', 'on', 'at', 'down', 'most', ""you've"", 'now', 'haven', ""shouldn't"", ""doesn't"", ""mightn't"", ""you're"", 're', 't', 'the', 'is', 'your', 'hadn', 'yours', 'an', ""you'll"", 'through', 'until', ""wouldn't"", 'and', 'has', 'were', ""weren't"", 'ours', 'here', 'above', 'but', 's', 'wouldn', 'before', 'do', 'to', 'why', 'out', 'we', 'or', 'any', 'for', 'in', 'ain', 'don', 'ourselves', 'these', ""aren't"", 'nor', 'am', 'again', 'its', 'will', 'a', 'under'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=4000, vectorizer=TfidfVectorizer(max_features=4000))), ('model', MultinomialNB())]",32000,"Vectorizer(max_features=4000, vectorizer=TfidfVectorizer(max_features=4000))",4000,"(1, 1)",TfidfVectorizer(max_features=4000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,4000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,71.34498801231385,1.86783019723251,17.304022598266602,0.3418593411448538,0.8593500000000001,0.0010302836381307862,0.8598036665774147,0.001048003683220016,0.8593850391417094,0.0010305865756458554,0.848925,0.002455351298694371,0.8496456814693726,0.002696900857804262,0.8489910527142408,0.002486671000598343
a07ee54cdaaa4a7382b6c2a6d7a7286f,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,MultinomialNB(),1.0,None,True,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'so', 'too', 'during', 'won', 'what', 'as', 'did', 'with', 'against', 'below', 'off', 've', 'be', ""don't"", 'aren', 'if', 'itself', 'own', 'i', ""it's"", 'o', 'same', ""wasn't"", 'it', 'both', 'd', 'where', 'that', ""mustn't"", ""won't"", 'about', 'other', 'didn', ""you'd"", 'there', 'which', 'mustn', 'himself', 'by', 'wasn', 'into', 'of', 'them', 'yourself', 'couldn', 'having', 'had', 'up', 'only', 'herself', 'just', 'very', 'who', 'her', 'hasn', 'me', 'this', 'themselves', 'while', 'how', ""that'll"", 'can', 'not', 'from', 'isn', 'being', 'our', 'once', 'been', 'have', 'such', 'they', 'his', 'because', 'over', 'mightn', 'does', 'few', 'y', ""should've"", 'myself', 'each', 'weren', 'my', 'he', 'no', ""hadn't"", 'whom', 'm', 'll', ""didn't"", 'shouldn', 'further', 'those', 'then', 'should', 'doing', 'theirs', 'yourselves', 'after', 'hers', ""couldn't"", ""isn't"", 'when', ""shan't"", 'than', 'ma', 'some', 'was', 'all', ""haven't"", 'more', 'him', 'she', 'doesn', ""hasn't"", ""she's"", 'needn', 'their', 'shan', 'you', ""needn't"", 'between', 'are', 'on', 'at', 'down', 'most', ""you've"", 'now', 'haven', ""shouldn't"", ""doesn't"", ""mightn't"", ""you're"", 're', 't', 'the', 'is', 'your', 'hadn', 'yours', 'an', ""you'll"", 'through', 'until', ""wouldn't"", 'and', 'has', 'were', ""weren't"", 'ours', 'here', 'above', 'but', 's', 'wouldn', 'before', 'do', 'to', 'why', 'out', 'we', 'or', 'any', 'for', 'in', 'ain', 'don', 'ourselves', 'these', ""aren't"", 'nor', 'am', 'again', 'its', 'will', 'a', 'under'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=2000, vectorizer=TfidfVectorizer(max_features=2000))), ('model', MultinomialNB())]",32000,"Vectorizer(max_features=2000, vectorizer=TfidfVectorizer(max_features=2000))",2000,"(1, 1)",TfidfVectorizer(max_features=2000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,2000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,71.5497917175293,0.7798391877236484,17.241633796691893,0.16433112902505337,0.8479625000000001,0.001259247046849825,0.8489980051226395,0.0013169867546565187,0.8480666091027571,0.001265988441036504,0.8412,0.003591308953571124,0.8421829244866584,0.0036911531296617054,0.8412951989456936,0.003611679106425959
f8baa491e7e142209b4f3b6b95873af4,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,MultinomialNB(),1.0,None,True,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'so', 'too', 'during', 'won', 'what', 'as', 'did', 'with', 'against', 'below', 'off', 've', 'be', ""don't"", 'aren', 'if', 'itself', 'own', 'i', ""it's"", 'o', 'same', ""wasn't"", 'it', 'both', 'd', 'where', 'that', ""mustn't"", ""won't"", 'about', 'other', 'didn', ""you'd"", 'there', 'which', 'mustn', 'himself', 'by', 'wasn', 'into', 'of', 'them', 'yourself', 'couldn', 'having', 'had', 'up', 'only', 'herself', 'just', 'very', 'who', 'her', 'hasn', 'me', 'this', 'themselves', 'while', 'how', ""that'll"", 'can', 'not', 'from', 'isn', 'being', 'our', 'once', 'been', 'have', 'such', 'they', 'his', 'because', 'over', 'mightn', 'does', 'few', 'y', ""should've"", 'myself', 'each', 'weren', 'my', 'he', 'no', ""hadn't"", 'whom', 'm', 'll', ""didn't"", 'shouldn', 'further', 'those', 'then', 'should', 'doing', 'theirs', 'yourselves', 'after', 'hers', ""couldn't"", ""isn't"", 'when', ""shan't"", 'than', 'ma', 'some', 'was', 'all', ""haven't"", 'more', 'him', 'she', 'doesn', ""hasn't"", ""she's"", 'needn', 'their', 'shan', 'you', ""needn't"", 'between', 'are', 'on', 'at', 'down', 'most', ""you've"", 'now', 'haven', ""shouldn't"", ""doesn't"", ""mightn't"", ""you're"", 're', 't', 'the', 'is', 'your', 'hadn', 'yours', 'an', ""you'll"", 'through', 'until', ""wouldn't"", 'and', 'has', 'were', ""weren't"", 'ours', 'here', 'above', 'but', 's', 'wouldn', 'before', 'do', 'to', 'why', 'out', 'we', 'or', 'any', 'for', 'in', 'ain', 'don', 'ourselves', 'these', ""aren't"", 'nor', 'am', 'again', 'its', 'will', 'a', 'under'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=1000, vectorizer=TfidfVectorizer(max_features=1000))), ('model', MultinomialNB())]",32000,"Vectorizer(max_features=1000, vectorizer=TfidfVectorizer(max_features=1000))",1000,"(1, 1)",TfidfVectorizer(max_features=1000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,1000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,71.38540797233581,1.0654631441037588,17.656924343109132,0.46879719313024737,0.8364,0.001168332144554796,0.8379091820854236,0.0011792675737992307,0.836564736719604,0.0011691354662714073,0.8326,0.005476883237754846,0.834060383403337,0.005653400080449531,0.8327631428621048,0.005509097778774596
4f9f703b2b114909a5c973d0217d93dd,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/size_evaluation.py,gonthierlucas,FINISHED,None,MultinomialNB(),1.0,None,True,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'so', 'too', 'during', 'won', 'what', 'as', 'did', 'with', 'against', 'below', 'off', 've', 'be', ""don't"", 'aren', 'if', 'itself', 'own', 'i', ""it's"", 'o', 'same', ""wasn't"", 'it', 'both', 'd', 'where', 'that', ""mustn't"", ""won't"", 'about', 'other', 'didn', ""you'd"", 'there', 'which', 'mustn', 'himself', 'by', 'wasn', 'into', 'of', 'them', 'yourself', 'couldn', 'having', 'had', 'up', 'only', 'herself', 'just', 'very', 'who', 'her', 'hasn', 'me', 'this', 'themselves', 'while', 'how', ""that'll"", 'can', 'not', 'from', 'isn', 'being', 'our', 'once', 'been', 'have', 'such', 'they', 'his', 'because', 'over', 'mightn', 'does', 'few', 'y', ""should've"", 'myself', 'each', 'weren', 'my', 'he', 'no', ""hadn't"", 'whom', 'm', 'll', ""didn't"", 'shouldn', 'further', 'those', 'then', 'should', 'doing', 'theirs', 'yourselves', 'after', 'hers', ""couldn't"", ""isn't"", 'when', ""shan't"", 'than', 'ma', 'some', 'was', 'all', ""haven't"", 'more', 'him', 'she', 'doesn', ""hasn't"", ""she's"", 'needn', 'their', 'shan', 'you', ""needn't"", 'between', 'are', 'on', 'at', 'down', 'most', ""you've"", 'now', 'haven', ""shouldn't"", ""doesn't"", ""mightn't"", ""you're"", 're', 't', 'the', 'is', 'your', 'hadn', 'yours', 'an', ""you'll"", 'through', 'until', ""wouldn't"", 'and', 'has', 'were', ""weren't"", 'ours', 'here', 'above', 'but', 's', 'wouldn', 'before', 'do', 'to', 'why', 'out', 'we', 'or', 'any', 'for', 'in', 'ain', 'don', 'ourselves', 'these', ""aren't"", 'nor', 'am', 'again', 'its', 'will', 'a', 'under'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=500, vectorizer=TfidfVectorizer(max_features=500))), ('model', MultinomialNB())]",32000,"Vectorizer(max_features=500, vectorizer=TfidfVectorizer(max_features=500))",500,"(1, 1)",TfidfVectorizer(max_features=500),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,500,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,71.83429260253907,0.6291477581791238,17.30349178314209,0.1446855698294323,0.8276125000000001,0.0010517099172300275,0.8290124478176253,0.0010381447001122969,0.8277410854294412,0.0010510071202967514,0.8254750000000002,0.0028781504477702288,0.8268629353167969,0.0033546937673746627,0.8256184108614459,0.0029450923634382117
