Run ID,Name,Source Type,Source Name,User,Status,memory,model,model__C,model__activation,model__alpha,model__batch_size,model__beta_1,model__beta_2,model__bootstrap,model__ccp_alpha,model__class_prior,model__class_weight,model__criterion,model__dual,model__early_stopping,model__epsilon,model__fit_intercept,model__fit_prior,model__hidden_layer_sizes,model__intercept_scaling,model__l1_ratio,model__learning_rate,model__learning_rate_init,model__loss,model__max_depth,model__max_features,model__max_fun,model__max_iter,model__max_leaf_nodes,model__max_samples,model__min_impurity_decrease,model__min_impurity_split,model__min_samples_leaf,model__min_samples_split,model__min_weight_fraction_leaf,model__momentum,model__multi_class,model__n_estimators,model__n_iter_no_change,model__n_jobs,model__nesterovs_momentum,model__oob_score,model__penalty,model__power_t,model__random_state,model__shuffle,model__solver,model__splitter,model__tol,model__validation_fraction,model__verbose,model__warm_start,preprocessor,preprocessor__normalization,preprocessor__stopwords,steps,training_size,vect,vect__max_features,vect__ngram_range,vect__vectorizer,vect__vectorizer__analyzer,vect__vectorizer__binary,vect__vectorizer__decode_error,vect__vectorizer__dtype,vect__vectorizer__encoding,vect__vectorizer__input,vect__vectorizer__lowercase,vect__vectorizer__max_df,vect__vectorizer__max_features,vect__vectorizer__min_df,vect__vectorizer__ngram_range,vect__vectorizer__norm,vect__vectorizer__preprocessor,vect__vectorizer__smooth_idf,vect__vectorizer__stop_words,vect__vectorizer__strip_accents,vect__vectorizer__sublinear_tf,vect__vectorizer__token_pattern,vect__vectorizer__tokenizer,vect__vectorizer__use_idf,vect__vectorizer__vocabulary,verbose,fit_time_mean,fit_time_std,predict_time_mean,predict_time_std,train_accuracy_score_mean,train_accuracy_score_std,train_f1_score_mean,train_f1_score_std,train_roc_auc_score_mean,train_roc_auc_score_std,valid_accuracy_score_mean,valid_accuracy_score_std,valid_f1_score_mean,valid_f1_score_std,valid_roc_auc_score_mean,valid_roc_auc_score_std
91e627e11d6b4a9296eb9571f87dd93c,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/validation.py,gonthierlucas,FINISHED,None,"LinearSVC(C=1, loss='hinge', max_iter=6000)",1,,,,,,,,,None,,True,,,True,,,1,,,,hinge,,,,6000,,,,,,,,,ovr,,,,,,l2,,None,,,,0.0001,,0,,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'hadn', ""hadn't"", 'we', 'he', 'couldn', 'being', ""you'd"", 'with', 'until', 'nor', 'own', 'ours', 'because', 'their', 'isn', 'those', 're', 'yourself', 'as', 'this', 'my', 'having', 'her', 'or', ""aren't"", 'hasn', 'they', 'theirs', 'that', 'have', 'themselves', 'below', 'am', 'herself', 'at', ""shouldn't"", ""mightn't"", 'a', 'has', 'other', 'why', 'to', 'himself', 'does', ""you've"", 'against', 'itself', 'him', 'while', ""wouldn't"", 'again', 'very', 'ain', 'above', 'it', 'so', 'on', 's', 'which', 'hers', 'before', 'over', 'his', 'shouldn', 'shan', 'both', 'up', 'too', 'by', 'when', 'its', 'for', 'all', 'd', 'during', 'haven', 'didn', 'needn', 'o', 'not', ""that'll"", 've', 'from', 'you', 'did', 'i', 'once', 'only', 'down', 'been', 'don', 'she', 'off', 'wasn', 'about', 'doesn', 'weren', 'won', 'few', ""weren't"", 'just', 'was', ""you're"", ""she's"", 'them', 'yours', 'same', 'can', 'any', 'more', 'should', 'than', 'be', ""shan't"", 'ourselves', 'aren', ""won't"", ""should've"", 'there', 'your', 'were', 'whom', ""wasn't"", 'after', 'will', 'll', 't', 'are', 'now', 'into', ""doesn't"", 'if', 'these', 'of', 'yourselves', 'myself', 'out', 'some', 'mustn', 'had', 'but', 'how', 'such', 'me', 'is', 'and', 'who', 'the', 'mightn', 'between', 'most', 'doing', 'our', 'do', 'then', 'ma', ""isn't"", 'under', 'further', ""you'll"", 'through', ""needn't"", ""didn't"", 'what', 'where', ""don't"", 'an', ""hasn't"", 'in', 'each', 'no', 'm', ""couldn't"", ""haven't"", 'here', 'wouldn', ""it's"", ""mustn't"", 'y'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=32000, vectorizer=TfidfVectorizer(max_features=32000))), ('model', LinearSVC(C=1, loss='hinge', max_iter=6000))]",32000,"Vectorizer(max_features=32000, vectorizer=TfidfVectorizer(max_features=32000))",32000,"(1, 1)",TfidfVectorizer(max_features=32000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,32000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,69.48937420845031,0.6549887744935944,18.657295274734498,0.3014114066049554,0.95305625,0.00046829144237323133,0.9531497192098801,0.000460786551739281,0.95307838952849,0.00046634914528313455,0.8941000000000001,0.0019516019061273653,0.8949162872596436,0.0020934716718411695,0.894249688413147,0.001991994950112198
c6ab781f64264d518fd5432b569313f2,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/validation.py,gonthierlucas,FINISHED,None,"LogisticRegression(C=1, max_iter=800, solver='sag')",1,,,,,,,,,None,,False,,,True,,,1,None,,,,,,,800,,,,,,,,,auto,,,None,,,l2,,None,,sag,,0.0001,,0,False,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'yourself', ""mightn't"", ""she's"", 'y', 'our', 'isn', 'an', 'where', 'that', 'wouldn', 'who', 'what', 'own', 'too', 'if', 'myself', 'so', 'won', 'over', 'hadn', 'its', 'doesn', 'am', 'were', 'each', ""weren't"", 'few', 'he', ""isn't"", ""wouldn't"", 'themselves', ""that'll"", 'it', 'as', ""shan't"", 'ours', 'himself', 'they', ""didn't"", 'shouldn', 'didn', 'been', 'itself', 'the', 'had', 'o', 'a', 'this', 'herself', 'having', 'other', ""hasn't"", 'such', ""hadn't"", 'against', 'how', 'at', 'doing', 'under', 've', 'mustn', ""shouldn't"", 'by', 'be', 'because', 'off', 'yours', 'to', ""you're"", 'theirs', 'shan', 'was', 'once', 'these', 'wasn', 'and', 'don', 'm', 'can', 'couldn', 'hasn', ""you'd"", 'between', 'will', 'just', 'up', 'before', 'on', ""it's"", 'has', 'now', 'her', 'do', 'than', 'll', 'from', 'after', 'nor', 'while', 'd', 'above', ""doesn't"", 'being', 'with', 'she', 'into', 'both', 'very', 'ma', 'why', ""you've"", 'mightn', 'their', 'is', ""should've"", 'my', 'ourselves', 'me', 'those', 'here', 'no', 'further', 'not', 'are', ""mustn't"", ""aren't"", ""won't"", 'out', 'you', 'weren', 'should', 'yourselves', 'aren', 're', 'i', 'does', 'which', 'in', 'them', 'hers', 't', ""couldn't"", 'during', 'ain', 'for', 'below', 'whom', 'through', 'same', 'did', 'when', 's', ""needn't"", 'we', ""don't"", 'down', 'again', 'haven', 'but', 'most', 'or', 'there', 'until', 'all', 'needn', 'only', 'your', 'any', 'him', ""wasn't"", ""you'll"", 'his', 'then', 'more', 'have', 'some', 'of', ""haven't"", 'about'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=32000, vectorizer=TfidfVectorizer(max_features=32000))), ('model', LogisticRegression(C=1, max_iter=800, solver='sag'))]",32000,"Vectorizer(max_features=32000, vectorizer=TfidfVectorizer(max_features=32000))",32000,"(1, 1)",TfidfVectorizer(max_features=32000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,32000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,69.1310646057129,0.48995008909812604,18.697587156295775,0.10676630757213292,0.9274687499999998,0.0006522197674097423,0.9279073601939954,0.0006241501628620213,0.927568576438363,0.000642375818545128,0.8904,0.0023417408054693164,0.8914767209667716,0.002425754111838322,0.8906303904506995,0.0023529413501913866
b447db2cfccf43c4b090235781738b61,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/validation.py,gonthierlucas,FINISHED,None,"MLPClassifier(alpha=0.001, hidden_layer_sizes=(50, 100, 200, 100, 50),
              learning_rate='adaptive', max_iter=300, solver='lbfgs')",,relu,0.001,auto,0.9,0.999,,,,,,,False,1e-08,,,"(50, 100, 200, 100, 50)",,,adaptive,0.001,,,,15000,300,,,,,,,,0.9,,,10,,True,,,0.5,None,True,lbfgs,,0.0001,0.1,False,False,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'they', 'against', 'should', 'can', 'your', 'until', 'them', 'd', 'were', 'wouldn', 'ain', 'you', 'at', 'or', 'mightn', 'once', 'themselves', 'do', ""wasn't"", 'am', 'to', 'if', 'nor', 'up', ""it's"", 'more', ""couldn't"", ""shouldn't"", 'having', ""haven't"", 'too', 'whom', ""you've"", 'how', 'under', 'are', 'while', 'ourselves', 'from', 'over', 'him', 'then', 'that', 'which', 'our', 'there', 'is', 'll', 'very', 'between', 'for', 'hadn', ""aren't"", 'did', 'be', 'my', 'of', 're', ""she's"", 'hers', 'their', 'his', 'yours', 'few', 'own', 'y', 'by', 'haven', 'needn', 'yourself', ""isn't"", 'not', ""wouldn't"", ""mightn't"", 'each', 'such', 'don', 'as', 'aren', 'out', 'was', 'all', 'some', 'm', 'through', 'we', 'about', 'on', 'with', 'an', 't', ""you're"", 'isn', 'o', 'any', 'doing', 'shan', 'does', 'its', 'has', 'just', 'it', ""mustn't"", ""that'll"", ""won't"", 'ours', 'mustn', ""didn't"", 'but', 'doesn', 'ma', 'her', ""hasn't"", 'after', ""doesn't"", 'no', 'further', 'a', 'above', 'what', 'myself', 'herself', 'into', 'why', 'those', 'only', 'me', 'other', 'didn', 'she', 'being', 'off', ""you'd"", 'now', 've', ""should've"", 'and', 'so', 'during', 'had', 'theirs', 'than', ""needn't"", 'the', 'down', ""you'll"", 'both', 's', 'before', ""weren't"", 'wasn', 'will', ""hadn't"", ""don't"", 'same', 'below', 'yourselves', 'because', 'couldn', 'here', 'weren', ""shan't"", 'hasn', 'i', 'been', 'most', 'he', 'these', 'himself', 'itself', 'have', 'won', 'where', 'who', 'shouldn', 'in', 'when', 'this', 'again'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=2000, vectorizer=TfidfVectorizer(max_features=2000))), ('model', MLPClassifier(alpha=0.001, hidden_layer_sizes=(50, 100, 200, 100, 50),
              learning_rate='adaptive', max_iter=300, solver='lbfgs'))]",32000,"Vectorizer(max_features=2000, vectorizer=TfidfVectorizer(max_features=2000))",2000,"(1, 1)",TfidfVectorizer(max_features=2000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,2000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,213.6214684009552,13.928534865406517,20.028109788894653,1.1946151732410462,0.8909937500000001,0.004785492398907366,0.8912783887675493,0.0046416303212137,0.891025156785475,0.004771176062595548,0.87385,0.0035234925854895536,0.8741945776229686,0.0035330750598889264,0.8738951771145066,0.003524986302198026
d17865e116ca403387ebbd5ef79e3e55,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/validation.py,gonthierlucas,FINISHED,None,"RandomForestClassifier(max_depth=14, max_features='sqrt', n_estimators=350)",,,,,,,True,0.0,,None,gini,,,,,,,,,,,,14,sqrt,,,None,None,0.0,None,1,2,0.0,,,350,,None,,False,,,None,,,,,,0,False,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'having', 'wouldn', ""you'd"", 'or', 'no', 'd', 'theirs', ""hadn't"", 'again', 'it', 'as', ""couldn't"", 'ain', ""shan't"", 'yourself', 'a', ""that'll"", 'weren', 'further', 'those', 'its', 'itself', 'by', 'yourselves', 'are', 'here', 'about', 'through', ""wasn't"", 'we', 'few', 'any', 'shan', ""aren't"", 'until', ""don't"", 'm', 'i', 'being', 'doing', 'my', 'how', ""haven't"", 'above', 'during', 'am', ""should've"", 'too', ""she's"", 'off', 'have', 'the', 'which', 'all', 'do', 'our', 'who', 'whom', 'with', 'now', 'mustn', 'such', 'very', 'ours', 'did', 'hers', 'then', 'him', 'me', 'his', 'not', 'hasn', 'these', 'was', 'himself', 'for', 'll', 'an', 'couldn', 'each', ""shouldn't"", 'that', 'does', ""it's"", 'herself', 'o', 'her', 'most', 'down', 'shouldn', 'and', 'this', 'if', 'on', 'other', ""mightn't"", 'yours', 'doesn', 'in', 'under', 'because', ""hasn't"", ""doesn't"", 'what', 'haven', ""you've"", 'against', 'when', 'don', 'from', 'had', 's', 'he', ""mustn't"", 't', 'once', ""won't"", 'wasn', 'while', 'than', 'but', 'just', 'be', 're', 'their', 'won', 'mightn', 'hadn', 'they', 'before', ""weren't"", 'has', 'of', 'you', 'more', 'themselves', 'isn', 'over', 'there', 'so', 'your', 'them', 'were', 'own', 'should', 'didn', ""wouldn't"", 'needn', 'below', 'ma', 'myself', 'y', 'ourselves', 'is', 'some', ""needn't"", 'into', 'at', 'she', 've', 'been', 'nor', 'to', 'between', 'after', 'same', ""didn't"", ""you're"", 'up', 'both', 'where', 'why', 'only', 'can', 'will', 'aren', 'out', ""isn't"", ""you'll""}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=32000, vectorizer=TfidfVectorizer(max_features=32000))), ('model', RandomForestClassifier(max_depth=14, max_features='sqrt', n_estimators=350))]",32000,"Vectorizer(max_features=32000, vectorizer=TfidfVectorizer(max_features=32000))",32000,"(1, 1)",TfidfVectorizer(max_features=32000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,32000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,79.817560338974,0.9580573108649921,19.687798833847047,0.7318555022203588,0.903725,0.0014735162028291543,0.9070175725882527,0.0014295469848728203,0.9059482426150292,0.001490817976793554,0.8427749999999999,0.007175217766730142,0.8471358139902427,0.006776222091014539,0.8440437537151017,0.007103807588593402
9d44f01d0fc045cc888ee4b600dea0f6,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/validation.py,gonthierlucas,FINISHED,None,"MultinomialNB(alpha=0.75, fit_prior=False)",,,0.75,,,,,,None,,,,,,,False,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'same', 'shouldn', 'this', 'did', 'because', 'and', ""that'll"", ""you've"", 'but', 'm', 'doesn', 'o', 've', 'wouldn', 'most', 'down', 'what', ""won't"", 'a', 'has', 'its', 'again', 'should', 'we', 'below', 'am', 'of', 'out', 'no', 'once', ""you'd"", ""isn't"", 'an', 'such', 'at', 'just', 'itself', 'for', 'y', 'didn', 'i', ""wasn't"", 'after', 'been', 'in', 'now', 't', ""didn't"", ""aren't"", 'before', 'ma', 'were', 'isn', 'll', ""you'll"", 'your', 'having', 'himself', 'off', 'herself', 'her', 'each', ""she's"", 'where', 'our', 'by', ""couldn't"", 'won', 'very', 'both', 'do', 'more', 'so', 'is', ""hadn't"", 'these', 'are', 'yourselves', 'if', 'theirs', 'does', 'they', 'only', 're', 'which', 'be', ""it's"", 'above', 'couldn', 'other', 'needn', 'shan', 'ours', ""don't"", 'you', 'hasn', ""shan't"", 'have', 'through', 'who', 'against', 'them', 'about', 'over', 'few', ""wouldn't"", ""shouldn't"", 'mightn', 'me', 'with', 'between', 'myself', ""hasn't"", ""mightn't"", 'wasn', 'their', 'the', 'nor', 'mustn', 'whom', 'themselves', 'into', 'she', 'had', 'yours', 'up', 'that', 'will', 'was', 'when', 'ain', 'his', ""should've"", 'ourselves', 'how', 'him', 'he', 'haven', 'hers', 'being', 's', 'until', 'all', 'not', 'or', 'during', 'any', 'those', 'here', 'then', 'own', 'than', 'yourself', 'aren', 'doing', 'further', ""haven't"", 'from', 'on', 'why', 'too', ""doesn't"", 'under', 'my', 'don', 'can', 'it', 'to', 'hadn', 'd', ""needn't"", ""mustn't"", 'weren', ""weren't"", 'some', ""you're"", 'while', 'there', 'as'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(vectorizer=TfidfVectorizer())), ('model', MultinomialNB(alpha=0.75, fit_prior=False))]",32000,Vectorizer(vectorizer=TfidfVectorizer()),None,"(1, 1)",TfidfVectorizer(),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,None,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,70.73293681144715,1.4551965948693817,19.257702827453613,0.6357245446796559,0.91573125,0.0009203854491461655,0.9145560278245541,0.0009164095982312055,0.9159786209881873,0.0009295506768414347,0.860425,0.005258683295274562,0.8581929586842867,0.005377769321140155,0.8607192315175235,0.0052555086229101105
e914170538fc4c05aa0d34315d39c659,,LOCAL,/Users/gonthierlucas/Desktop/DS_project/IMDB_reviews/experimentation/validation.py,gonthierlucas,FINISHED,None,DecisionTreeClassifier(max_depth=14),,,,,,,,0.0,,None,gini,,,,,,,,,,,,14,None,,,None,,0.0,None,1,2,0.0,,,,,,,,,,None,,,best,,,,,"TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})",2,"{'didn', 'before', 'again', 'by', 'our', 'theirs', 'that', 'it', 'why', 'because', 'your', 'from', 'she', 'some', 'about', 'yours', 'only', 'am', ""that'll"", 'shan', 'their', 'below', 'how', 'itself', ""weren't"", 'having', ""hasn't"", 'between', 'he', 'herself', 'up', 'haven', 'of', ""mustn't"", 'are', 'what', 'to', 'me', ""wasn't"", 'should', 'ma', 'her', 'under', 'whom', 'mustn', 'most', 'ain', 've', 'being', 'mightn', 'or', 'through', 'an', 'any', 'no', ""you're"", 'against', 'who', 'himself', 'above', 'there', 'll', ""haven't"", 'been', 'as', 'when', 'each', 'after', 'a', 'doing', 'on', 'him', 'y', 'has', 'too', ""she's"", 'while', 'now', 'further', ""shouldn't"", 'is', 'shouldn', 'over', 'yourself', 'but', 'themselves', 'where', 'more', 'hadn', ""shan't"", 'at', 'off', 'weren', 'was', 'o', 'these', ""needn't"", 'then', 'don', 'and', 'hers', 'had', 'not', 'both', 'with', 'them', 'those', 'can', 'd', 'wouldn', 'aren', ""didn't"", 'we', 'very', 'once', 'just', 'wasn', 'until', 'have', ""mightn't"", ""you'd"", 'won', 're', 'out', ""should've"", 't', 'same', 'm', 'own', ""aren't"", 'ourselves', 'did', 'such', 'will', 'during', 'nor', 'you', ""couldn't"", 'needn', 'into', 'for', 'i', 'myself', 'do', ""you'll"", 'they', 'ours', ""wouldn't"", 's', 'the', ""doesn't"", 'few', ""won't"", 'be', ""hadn't"", ""isn't"", 'other', ""you've"", 'couldn', 'yourselves', 'so', 'my', 'his', 'in', 'which', 'than', 'were', 'hasn', 'isn', 'down', 'this', 'here', 'does', 'all', 'its', ""don't"", ""it's"", 'doesn', 'if'}","[('preprocessor', TextPreprocessor(normalization=2,
                 stopwords={'a', 'about', 'above', 'after', 'again', 'against',
                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',
                            'aren', ""aren't"", 'as', 'at', 'be', 'because',
                            'been', 'before', 'being', 'below', 'between',
                            'both', 'but', 'by', 'can', 'couldn', ""couldn't"", ...})), ('vect', Vectorizer(max_features=4000, vectorizer=TfidfVectorizer(max_features=4000))), ('model', DecisionTreeClassifier(max_depth=14))]",32000,"Vectorizer(max_features=4000, vectorizer=TfidfVectorizer(max_features=4000))",4000,"(1, 1)",TfidfVectorizer(max_features=4000),word,False,strict,<class 'numpy.float64'>,utf-8,content,True,1.0,4000,1,"(1, 1)",l2,None,True,None,None,False,(?u)\b\w\w+\b,None,True,None,False,73.3230619430542,0.4720287439378343,18.376493072509767,0.16861935028311517,0.80866875,0.002457498092573003,0.8249609932513827,0.0025535787047236583,0.8202869395972326,0.003129889271713001,0.735825,0.0048609412668741475,0.7585633027432863,0.005315008578354345,0.7449535419602843,0.005816053660429877
